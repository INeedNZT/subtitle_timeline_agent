import os
import re
import json
import gradio as gr
from typing import List, Tuple, Dict, Iterator
import threading
from queue import Queue, Empty

# LangChain imports
from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage


# Import our custom tools and templates
from tools import (
    list_video_files, get_video_info, extract_video_audio,
    extract_subtitle, sync_subtitles, cleanup_temp,
    list_temp_files, check_external_subtitle, copy_to_temp,
    cleanup_subtitle
)
# Create a mapping from tool names to the actual functions
tool_map = {
    "list_video_files": list_video_files,
    "get_video_info": get_video_info,
    "extract_video_audio": extract_video_audio,
    "extract_subtitle": extract_subtitle,
    "sync_subtitles": sync_subtitles,
    "cleanup_temp": cleanup_temp,
    "list_temp_files": list_temp_files,
    "check_external_subtitle": check_external_subtitle,
    "copy_to_temp": copy_to_temp,
    "cleanup_subtitle": cleanup_subtitle,
}

# --- LangChain Setup ---
api_key = os.getenv("OPENAI_API_KEY", "")
base_url = os.getenv("OPENAI_BASE_URL", "https://api.deepseek.com/v1")
model = os.getenv("MODEL_NAME", "deepseek-chat")

llm = ChatOpenAI(model=model, temperature=0, api_key=api_key, base_url=base_url)
tools = [
    list_video_files, get_video_info, extract_video_audio,
    extract_subtitle, sync_subtitles, cleanup_temp,
    list_temp_files, check_external_subtitle, copy_to_temp,
    cleanup_subtitle
]

prompt = ChatPromptTemplate.from_messages([
    ("system", """ä½ æ˜¯ä¸€ä¸ªç”¨äºæ‰¹é‡å¤„ç†è§†é¢‘å­—å¹•çš„AIé¡¹ç›®ç»ç†ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤è½¬æ¢æˆä¸€ä¸ªç»“æ„åŒ–çš„ã€å¯æ‰§è¡Œçš„JSONä»»åŠ¡è®¡åˆ’ã€‚

ä½ çš„å·¥ä½œæµç¨‹å¦‚ä¸‹ï¼š
1. **ç†è§£ç”¨æˆ·æ„å›¾**: åˆ†æç”¨æˆ·æƒ³è¦å¤„ç†å“ªäº›æ–‡ä»¶ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨ `list_video_files` å·¥å…·ï¼‰ä»¥åŠå…·ä½“çš„æ“ä½œï¼ˆä¾‹å¦‚ï¼Œæå–éŸ³è§†é¢‘ã€åŒæ­¥å­—å¹•ç­‰ï¼‰
2. **æ”¶é›†ä¿¡æ¯**: å¯¹äºæ¯ä¸ªæ‰¾åˆ°çš„è§†é¢‘ï¼Œé¦–å…ˆä½¿ç”¨ `check_external_subtitle` æ¥æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¤–éƒ¨å­—å¹•æ–‡ä»¶
   - å¦‚æœå­˜åœ¨å¤–éƒ¨å­—å¹•ï¼Œä¼˜å…ˆä½¿ç”¨å¤–éƒ¨å­—å¹•ï¼Œç”¨ `copy_to_temp` å°†å…¶å¤åˆ¶åˆ°ä¸´æ—¶ç›®å½•ï¼Œç„¶åç›´æ¥è¿›è¡ŒåŒæ­¥
   - å¦‚æœä¸å­˜åœ¨å¤–éƒ¨å­—å¹•ï¼Œæ‰ä½¿ç”¨ `get_video_info` å’Œ `extract_subtitle` æ¥æå–å†…ç½®å­—å¹•
   - åªæœ‰å½“ç”¨æˆ·æ˜ç¡®è¦æ±‚æå–å†…ç½®å­—å¹•æ—¶ï¼ˆå¦‚ï¼š"æå–å­—å¹•"ã€"ç”¨è§†é¢‘é‡Œçš„å­—å¹•"ç­‰ï¼‰ï¼Œæ‰è·³è¿‡å¤–éƒ¨å­—å¹•æ£€æŸ¥
3. **ç”ŸæˆJSONè®¡åˆ’**: ä½ çš„æœ€ç»ˆè¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å«ï¼š
   - `tasks` åˆ—è¡¨ï¼šæ¯ä¸ªä»»åŠ¡åŒ…å« `source_file` å’Œ `steps` åˆ—è¡¨
   - `global_steps` åˆ—è¡¨ï¼šåœ¨æ‰€æœ‰è§†é¢‘å¤„ç†å®Œæ¯•åæ‰§è¡Œçš„å…¨å±€æ¸…ç†æ­¥éª¤ï¼ˆé»˜è®¤ `cleanup_subtitle` ï¼Œåªæœ‰å½“ç”¨æˆ·æ˜ç¡®è¦æ±‚æ‰ `cleanup_temp`ï¼‰

**JSONè¾“å‡ºæ ¼å¼ç¤ºä¾‹**:
```json
{{
  "tasks": [
    {{
      "source_file": "Episode/S01E01.mp4",
      "steps": [
        {{"tool": "extract_video_audio", "params": {{"input_path": "Episode/S01E01.mp4", "output_filename": "S01E01.mp4", "video_stream_index": 0, "audio_stream_index": 1}}}},
        {{"tool": "copy_to_temp", "params": {{"file_path": "Episode/S01E01.srt"}}}},
        {{"tool": "sync_subtitles", "params": {{"video_filename": "S01E01.mp4", "subtitle_filename": "S01E01.srt", "output_subtitle_name": "S01E01_synced.srt"}}}}
      ]
    }}
  ],
  "global_steps": [
    {{"tool": "cleanup_subtitle", "params": {{"temp_dir": "tmp"}}}}
  ]
}}
```

è¯·ä¸¥æ ¼éµå®ˆæ­¤JSONæ ¼å¼ã€‚ä¸è¦æ‰§è¡Œå®é™…çš„å·¥å…·æ“ä½œï¼Œåªéœ€è§„åˆ’å‡ºè¿™äº›æ­¥éª¤å³å¯ã€‚
"""),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad"),
])

agent = create_openai_tools_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

def agent_planner_thread(message: str, chat_history: List, queue: Queue):
    """
    Target for the agent planning thread.
    The agent's goal is to produce a JSON plan.
    """
    try:
        # We don't need the streaming callback here for the planner
        response = agent_executor.invoke(
            {"input": message, "chat_history": chat_history}
        )
        queue.put({"type": "plan", "content": response.get("output", "")})
    except Exception as e:
        queue.put({"type": "error", "content": f"æŠ±æ­‰ï¼Œè§„åˆ’æ—¶é‡åˆ°é”™è¯¯: {e}"})


def task_executor_thread(tasks: List[Dict], global_steps: List[Dict], queue: Queue):
    """
    Target for the task execution thread.
    Executes the plan provided by the agent.
    """
    total_tasks = len(tasks)
    for i, task in enumerate(tasks):
        source_file = task.get("source_file", "æœªçŸ¥æ–‡ä»¶")
        queue.put({
            "type": "progress",
            "content": f"å¥½çš„ï¼Œæˆ‘ä»¬å¼€å§‹å¤„ç†ç¬¬ {i+1} ä¸ªä»»åŠ¡ï¼Œç›®æ ‡æ–‡ä»¶æ˜¯ `{source_file}`ã€‚\n"
        })

        for step_idx, step in enumerate(task.get("steps", [])):
            tool_name = step.get("tool")
            params = step.get("params", {})
            
            # ç¿»è¯‘å·¥å…·åç§°ä¸ºæ›´è‡ªç„¶çš„è¯­è¨€
            tool_translation = {
                "copy_to_temp": "å¤åˆ¶æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•",
                "extract_video_audio": "æå–è§†é¢‘å’ŒéŸ³é¢‘æµ",
                "extract_subtitle": "æå–å­—å¹•æ–‡ä»¶",
                "sync_subtitles": "åŒæ­¥å­—å¹•æ—¶é—´è½´",
                "cleanup_temp": "æ¸…ç†ä¸´æ—¶æ–‡ä»¶",
                "cleanup_subtitle": "æ¸…ç†å­—å¹•æ–‡ä»¶"
            }
            step_description = tool_translation.get(tool_name, f"æ‰§è¡Œ `{tool_name}`")

            queue.put({"type": "progress", "content": f"\n- **ç¬¬ {step_idx + 1} æ­¥**: {step_description}..."})
            
            if tool_name in tool_map:
                try:
                    tool_function = tool_map[tool_name]
                    result = tool_function.invoke(params)
                    
                    if isinstance(result, dict) and result.get("status") == "success":
                        queue.put({"type": "progress", "content": f" âœ… æˆåŠŸï¼"})
                        if "message" in result:
                             queue.put({"type": "progress", "content": f" `{result['message']}`"})
                    elif isinstance(result, dict) and result.get("status") == "error":
                        error_message = result.get('message', 'æœªçŸ¥é”™è¯¯')
                        queue.put({"type": "progress", "content": f" âŒ å¤±è´¥ã€‚é”™è¯¯ä¿¡æ¯: {error_message}"})
                        break
                    else:
                        result_str = json.dumps(result, ensure_ascii=False, indent=2) if isinstance(result, dict) else str(result)
                        queue.put({"type": "progress", "content": f" âœ… æ“ä½œå®Œæˆï¼Œè¿”å›ä¿¡æ¯ï¼š\n```{result_str}```"})

                except Exception as e:
                    queue.put({"type": "progress", "content": f" âŒ å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}"})
                    break
            else:
                queue.put({"type": "progress", "content": f" âš ï¸ **è­¦å‘Š**: æœªæ‰¾åˆ°åä¸º `{tool_name}` çš„å·¥å…·ã€‚"})
        
        queue.put({"type": "progress", "content": f"\n\n---\n"})

    # æ‰§è¡Œå…¨å±€æ¸…ç†æ­¥éª¤ï¼ˆä»…æ‰§è¡Œä¸€æ¬¡ï¼‰
    if global_steps:
        queue.put({"type": "progress", "content": "\n### ğŸ§¹ å…¨å±€æ¸…ç†é˜¶æ®µ\n"})
        for step_idx, step in enumerate(global_steps):
            tool_name = step.get("tool")
            params = step.get("params", {})
            
            tool_translation = {
                "cleanup_temp": "æ¸…ç†ä¸´æ—¶æ–‡ä»¶",
                "cleanup_subtitle": "æ¸…ç†å­—å¹•æ–‡ä»¶"
            }
            step_description = tool_translation.get(tool_name, f"æ‰§è¡Œ `{tool_name}`")
            
            queue.put({"type": "progress", "content": f"\n- **å…¨å±€æ­¥éª¤ {step_idx + 1}**: {step_description}..."})
            
            if tool_name in tool_map:
                try:
                    tool_function = tool_map[tool_name]
                    result = tool_function.invoke(params)
                    
                    if isinstance(result, dict) and result.get("status") == "success":
                        queue.put({"type": "progress", "content": f" âœ… æˆåŠŸï¼"})
                        if "message" in result:
                             queue.put({"type": "progress", "content": f" `{result['message']}`"})
                    elif isinstance(result, dict) and result.get("status") == "error":
                        error_message = result.get('message', 'æœªçŸ¥é”™è¯¯')
                        queue.put({"type": "progress", "content": f" âŒ å¤±è´¥ã€‚é”™è¯¯ä¿¡æ¯: {error_message}"})
                    else:
                        result_str = json.dumps(result, ensure_ascii=False, indent=2) if isinstance(result, dict) else str(result)
                        queue.put({"type": "progress", "content": f" âœ… æ“ä½œå®Œæˆï¼Œè¿”å›ä¿¡æ¯ï¼š\n```{result_str}```"})
                except Exception as e:
                    queue.put({"type": "progress", "content": f" âŒ å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}"})
            else:
                queue.put({"type": "progress", "content": f" âš ï¸ **è­¦å‘Š**: æœªæ‰¾åˆ°åä¸º `{tool_name}` çš„å·¥å…·ã€‚"})

    queue.put({"type": "done", "content": "æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæ¯•ï¼"})


def predict(message: str, history: List[Tuple[str, str]]) -> Iterator[Tuple[List[Tuple[str, str]], str, str]]:
    # æ­£ç¡®æ„å»ºchat_historyï¼šä»historyä¸­æå–(user_message, ai_response)å¯¹
    chat_history = []
    for user_msg, ai_msg in history:
        if user_msg:  # ç”¨æˆ·æ¶ˆæ¯ä¸ä¸ºç©º
            chat_history.append(HumanMessage(content=user_msg))
        if ai_msg:  # AIå“åº”ä¸ä¸ºç©º
            chat_history.append(AIMessage(content=ai_msg))
    
    q = Queue()
    log_content = "### é˜¶æ®µä¸€ï¼šä»»åŠ¡è§„åˆ’\n"
    yield history[:-1] + [[message, "æ­£åœ¨æ€è€ƒä¸­ï¼Œè¯·ç¨å€™..."]], log_content

    # --- Planning Stage ---
    planner_thread = threading.Thread(target=agent_planner_thread, args=(message, chat_history, q))
    planner_thread.start()

    plan_json_str = ""
    while planner_thread.is_alive() or not q.empty():
        try:
            event = q.get(timeout=0.1)
            if event["type"] == "plan":
                plan_json_str = event["content"]
                break
            elif event["type"] == "error":
                yield history[:-1] + [[message, event["content"]]], log_content
                return
        except Empty:
            continue
    
    planner_thread.join()

    # ä½¿ç”¨æ­£åˆ™æå– JSON å†…å®¹
    json_match = re.search(r'\{.*\}', plan_json_str, re.DOTALL)
    
    if json_match:
        plan_json_str = json_match.group()
        log_content += f"âœ… AIç”Ÿæˆäº†ä»»åŠ¡è®¡åˆ’ã€‚\n```json\n{plan_json_str}\n```\n"
        yield history[:-1] + [[message, "è§„åˆ’å®Œæˆï¼Œå‡†å¤‡æ‰§è¡Œ..."]], log_content
        
        try:
            plan = json.loads(plan_json_str)
        except json.JSONDecodeError:
            log_content += f"âŒ JSONè§£æå¤±è´¥ï¼ŒåŸå§‹å­—ç¬¦: {plan_json_str}\n"
            yield history[:-1] + [[message, "AIç”Ÿæˆçš„è®¡åˆ’æ ¼å¼æœ‰è¯¯ï¼Œæ— æ³•è§£æã€‚"]], log_content
            return
    else:
        # å¦‚æœæ‰¾ä¸åˆ° JSON ç»“æ„ï¼Œè¯´æ˜ LLM ç›´æ¥å›å¤äº†å¯¹è¯
        log_content += f"âš ï¸ AIæœªç”Ÿæˆç»“æ„åŒ–è®¡åˆ’ï¼Œ{plan_json_str}\n"
        # å°è¯•ç›´æ¥æŠŠ LLM çš„å›å¤å±•ç¤ºç»™ç”¨æˆ·
        yield history[:-1] + [[message, f"AIæœªç”Ÿæˆæ‰§è¡Œè®¡åˆ’ï¼Œ{plan_json_str}"]], log_content
        return
    
    yield history[:-1] + [[message, "è§„åˆ’å®Œæˆï¼Œå‡†å¤‡æ‰§è¡Œ..."]], log_content

    # --- Execution Stage ---
    try:
        plan = json.loads(plan_json_str)
        tasks = plan.get("tasks", [])
        global_steps = plan.get("global_steps", [])
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•æœ‰æ•ˆä»»åŠ¡æˆ–å…¨å±€æ­¥éª¤
        if not tasks and not global_steps:
            yield history[:-1] + [[message, "è®¡åˆ’ä¸­æ²¡æœ‰å‘ç°ä»»ä½•æœ‰æ•ˆä»»åŠ¡æˆ–å…¨å±€æ­¥éª¤ã€‚"]], log_content
            return

    except json.JSONDecodeError:
        yield history[:-1] + [[message, "æ— æ³•è§£æAIç”Ÿæˆçš„ä»»åŠ¡è®¡åˆ’ï¼Œè¯·æ£€æŸ¥æ ¼å¼ã€‚"]], log_content
        return

    log_content += "\n### é˜¶æ®µäºŒï¼šä»»åŠ¡æ‰§è¡Œ\n"
    
    executor_thread = threading.Thread(target=task_executor_thread, args=(tasks, global_steps, q))
    executor_thread.start()

    final_response = "ä»»åŠ¡æ‰§è¡Œä¸­..."
    while executor_thread.is_alive() or not q.empty():
        try:
            event = q.get(timeout=0.1)
            if event["type"] == "progress":
                log_content += event["content"]
            elif event["type"] == "done":
                final_response = event["content"]
            
            yield history[:-1] + [[message, final_response]], log_content
        except Empty:
            continue
    
    executor_thread.join()
    yield history[:-1] + [[message, final_response]], log_content

# --- Gradio Interface ---
with gr.Blocks(
    title="âœï¸ å­—å¹•æ—¶é—´çº¿æ ¡æ­£åŠ©æ‰‹",
    theme="soft",
    css="""
    .markdown-container {
        padding: 10px 5px !important;
    }
    .markdown-label {
        font-weight: bold;
        margin-bottom: 10px;
        display: block;
    }
""",
) as app:

    gr.Markdown("# âœï¸ å­—å¹•æ—¶é—´çº¿æ ¡æ­£åŠ©æ‰‹\nè·Ÿæˆ‘è¯´æƒ³è¦æ ¡æ­£å­—å¹•çš„è§†é¢‘åç§°ï¼Œæˆ‘å°†ä¸ºä½ æœç´¢å¹¶è¿›è¡Œæå–éŸ³é¢‘æµã€å­—å¹•æ–‡ä»¶å’Œæ—¶é—´çº¿çš„æ ¡æ­£ã€ä»¥åŠåç»­çš„æ¸…ç†å·¥ä½œ...")

    with gr.Row():
        with gr.Column(scale=2):
            chatbot = gr.Chatbot(label="å¯¹è¯çª—å£", height=500, show_copy_button=True)
            
            with gr.Row():
                user_input = gr.Textbox(
                    show_label=False, 
                    placeholder="æŒ‡ä»¤å¦‚ï¼š'å¸®æˆ‘åŒæ­¥ç‹‚é£™ç¬¬ä¸€é›†å­—å¹•æ—¶é—´è½´...'", 
                    scale=8, 
                    container=False
                )
                submit_btn = gr.Button("å‘é€", scale=1, variant="primary")
                clear_btn = gr.Button("æ¸…é™¤", scale=1, variant="secondary")

        with gr.Column(scale=1):
            gr.HTML("<div class='markdown-label'>ğŸ“ å¤„ç†æ­¥éª¤</div>")
            log_display = gr.Markdown(elem_classes=["markdown-container"], elem_id="log-box", height=400)
            
            # è‡ªåŠ¨æ»šåŠ¨è„šæœ¬ï¼šåªåœ¨ç”¨æˆ·ä½äºåº•éƒ¨é™„è¿‘æ—¶è‡ªåŠ¨æ»šåŠ¨
            scroll_js = """
            function() {
                const el = document.querySelector('#log-box .markdown-container');
                if (el) {
                    // æ£€æŸ¥ç”¨æˆ·æ˜¯å¦åœ¨åº•éƒ¨é™„è¿‘ï¼ˆè·ç¦»åº•éƒ¨5pxä»¥å†…ï¼‰
                    const isNearBottom = el.scrollHeight - el.scrollTop - el.clientHeight < 5;
                    
                    // åªæœ‰åœ¨æ¥è¿‘åº•éƒ¨æ—¶æ‰è‡ªåŠ¨æ»šåŠ¨
                    if (isNearBottom) {
                        setTimeout(() => {
                            el.scrollTop = el.scrollHeight;
                        }, 50);
                    }
                }
            }
            """
            log_display.change(fn=None, inputs=None, outputs=None, js=scroll_js)

    def chat_fn(message: str, chat_history: List[Tuple[str, str]]) -> Iterator[Tuple[List[Tuple[str, str]], str, str]]:
        chat_history.append((message, ""))
        
        # ç¦ç”¨è¾“å…¥æ¡†å’ŒæŒ‰é’®
        yield chat_history, "", gr.update(interactive=False), gr.update(interactive=False), gr.update(interactive=False)

        for updated_history, logs in predict(message, chat_history):
            chat_history = updated_history
            yield chat_history, logs, gr.update(interactive=False), gr.update(interactive=False), gr.update(interactive=False)
        
        # å¤„ç†å®Œæˆåï¼Œå¯ç”¨è¾“å…¥æ¡†å’ŒæŒ‰é’®
        yield chat_history, logs, gr.update(interactive=True), gr.update(interactive=True), gr.update(interactive=True)

    submit_btn.click(
        chat_fn,
        inputs=[user_input, chatbot],
        outputs=[chatbot, log_display, user_input, submit_btn, clear_btn]
    ).then(lambda: gr.update(value=""), None, [user_input], queue=False)

    user_input.submit(
        chat_fn,
        inputs=[user_input, chatbot],
        outputs=[chatbot, log_display, user_input, submit_btn, clear_btn]
    ).then(lambda: gr.update(value=""), None, [user_input], queue=False)

    clear_btn.click(
        fn=lambda: ([], ""),
        inputs=[],
        outputs=[chatbot, log_display],
        queue=False
    )

if __name__ == "__main__":
    app.launch(server_name="0.0.0.0", server_port=80)
