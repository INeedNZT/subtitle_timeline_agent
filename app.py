import os
import re
import json
import gradio as gr
from typing import List, Tuple
import threading
import uuid
import time

# LangChain imports
from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage


# Import our custom tools and templates
from tools import (
    list_video_files, get_video_info, extract_video_audio,
    extract_subtitle, sync_subtitles, cleanup_temp,
    list_temp_files, check_external_subtitle, copy_to_temp,
    cleanup_subtitle
)
# Create a mapping from tool names to the actual functions
tool_map = {
    "list_video_files": list_video_files,
    "get_video_info": get_video_info,
    "extract_video_audio": extract_video_audio,
    "extract_subtitle": extract_subtitle,
    "sync_subtitles": sync_subtitles,
    "cleanup_temp": cleanup_temp,
    "list_temp_files": list_temp_files,
    "check_external_subtitle": check_external_subtitle,
    "copy_to_temp": copy_to_temp,
    "cleanup_subtitle": cleanup_subtitle,
}

# --- LangChain Setup ---
api_key = os.getenv("OPENAI_API_KEY", "")
base_url = os.getenv("OPENAI_BASE_URL", "https://api.deepseek.com/v1")
model = os.getenv("MODEL_NAME", "deepseek-chat")

llm = ChatOpenAI(model=model, temperature=0, api_key=api_key, base_url=base_url)
tools = [
    list_video_files, get_video_info, extract_video_audio,
    extract_subtitle, sync_subtitles, cleanup_temp,
    list_temp_files, check_external_subtitle, copy_to_temp,
    cleanup_subtitle
]

prompt = ChatPromptTemplate.from_messages([
    ("system", """ä½ æ˜¯ä¸€ä¸ªç”¨äºæ‰¹é‡å¤„ç†è§†é¢‘å­—å¹•çš„AIé¡¹ç›®ç»ç†ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤è½¬æ¢æˆä¸€ä¸ªç»“æ„åŒ–çš„ã€å¯æ‰§è¡Œçš„JSONä»»åŠ¡è®¡åˆ’ã€‚

ä½ çš„å·¥ä½œæµç¨‹å¦‚ä¸‹ï¼š
1. **ç†è§£ç”¨æˆ·æ„å›¾**: åˆ†æç”¨æˆ·æƒ³è¦å¤„ç†å“ªäº›æ–‡ä»¶ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨ `list_video_files` å·¥å…·ï¼‰
2. **æ”¶é›†ä¿¡æ¯**: å¯¹äºæ¯ä¸ªæ‰¾åˆ°çš„è§†é¢‘ï¼Œé™¤éç”¨æˆ·æ˜ç¡®è¦æ±‚ï¼Œå¦åˆ™ä½¿ç”¨ `check_external_subtitle` æ¥æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¤–éƒ¨å­—å¹•æ–‡ä»¶
   - å¦‚æœå­˜åœ¨å¤–éƒ¨å­—å¹•ï¼Œä¼˜å…ˆä½¿ç”¨å¤–éƒ¨å­—å¹•ï¼Œç”¨ `copy_to_temp` å°†å…¶å¤åˆ¶åˆ°ä¸´æ—¶ç›®å½•
   - å¦‚æœä¸å­˜åœ¨å¤–éƒ¨å­—å¹•ï¼Œæ‰ä½¿ç”¨ `get_video_info` å’Œ `extract_subtitle` æ¥æå–å†…ç½®å­—å¹•
3. **æå–éŸ³è§†é¢‘**: ä½¿ç”¨ `get_video_info` åˆ†æè§†é¢‘å…ƒæ•°æ®ï¼Œé€šè¿‡ `extract_video_audio` æ¥æ ¹æ®æµç´¢å¼•æå–éŸ³è§†é¢‘
4. **åŒæ­¥å­—å¹•æ—¶é—´è½´**: é€šè¿‡ `sync_subtitles` å°†è§†é¢‘æ–‡ä»¶å’Œå­—å¹•æ–‡ä»¶è¿›è¡Œæ—¶é—´è½´çš„æ ¡æ­£
5. **ç”ŸæˆJSONè®¡åˆ’**: ä½ çš„æœ€ç»ˆè¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å«ï¼š
   - `tasks` åˆ—è¡¨ï¼šæ¯ä¸ªä»»åŠ¡åŒ…å« `source_file` å’Œ `steps` åˆ—è¡¨
   - `global_steps` åˆ—è¡¨ï¼šåœ¨æ‰€æœ‰è§†é¢‘å¤„ç†å®Œæ¯•åæ‰§è¡Œçš„å…¨å±€æ¸…ç†æ­¥éª¤ï¼ˆé»˜è®¤ `cleanup_subtitle` ï¼Œåªæœ‰å½“ç”¨æˆ·æ˜ç¡®è¦æ±‚æ‰ `cleanup_temp`ï¼‰

**JSONè¾“å‡ºæ ¼å¼ç¤ºä¾‹**:
```json
{{
  "tasks": [
    {{
      "source_file": "Episode/S01E01.mp4",
      "steps": [
        {{"tool": "extract_video_audio", "params": {{"input_path": "Episode/S01E01.mp4", "output_filename": "S01E01.mp4", "video_stream_index": 0, "audio_stream_index": 1}}}},
        {{"tool": "copy_to_temp", "params": {{"file_path": "Episode/S01E01.srt"}}}},
        {{"tool": "sync_subtitles", "params": {{"video_filename": "S01E01.mp4", "subtitle_filename": "S01E01.srt", "output_subtitle_name": "S01E01_synced.srt"}}}}
      ]
    }}
  ],
  "global_steps": [
    {{"tool": "cleanup_subtitle", "params": {{"temp_dir": "tmp"}}}}
  ]
}}
```

è¯·ä¸¥æ ¼éµå®ˆæ­¤JSONæ ¼å¼ã€‚ä¸è¦æ‰§è¡Œå®é™…çš„å·¥å…·æ“ä½œï¼Œåªéœ€è§„åˆ’å‡ºè¿™äº›æ­¥éª¤å³å¯ã€‚
"""),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad"),
])

agent = create_openai_tools_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

# --- Global Job Management ---
JOBS = {}
JOBS_LOCK = threading.Lock()

def log_to_job(job_id: str, content: str, message_type: str = "progress"):
    """Helper to safely append logs to a job."""
    with JOBS_LOCK:
        if job_id in JOBS:
            JOBS[job_id]["logs"].append(content)
            if message_type in ["done", "error"]:
                JOBS[job_id]["status"] = message_type
                # å¦‚æœæ˜¯å®Œæˆæˆ–é”™è¯¯ï¼Œä¹Ÿå°†æ­¤ä½œä¸ºæœ€ç»ˆå“åº”è®°å½•ï¼Œä»¥ä¾¿æ›´æ–°UI
                JOBS[job_id]["final_response"] = content if message_type == "done" else f"å‡ºé”™: {content}"

def background_task_runner(job_id: str, message: str, chat_history_tuples: List[Tuple[str, str]]):
    """
    The main function running in a background thread.
    It handles both planning and execution phases.
    """
    # Construct LangChain history objects
    chat_history = []
    for user_msg, ai_msg in chat_history_tuples:
        if user_msg:
            chat_history.append(HumanMessage(content=user_msg))
        if ai_msg:
            chat_history.append(AIMessage(content=ai_msg))

    log_to_job(job_id, "### é˜¶æ®µä¸€ï¼šä»»åŠ¡è§„åˆ’\næ­£åœ¨æ€è€ƒä¸­ï¼Œè¯·ç¨å€™...\n")

    # --- Phase 1: Planning ---
    try:
        response = agent_executor.invoke(
            {"input": message, "chat_history": chat_history}
        )
        plan_json_str = response.get("output", "")
    except Exception as e:
        log_to_job(job_id, f"è§„åˆ’é˜¶æ®µå‘ç”Ÿå¼‚å¸¸: {e}", "error")
        return

    # Parse JSON Plan
    json_match = re.search(r'\{.*\}', plan_json_str, re.DOTALL)
    if json_match:
        plan_json_str = json_match.group()
        log_to_job(job_id, f"âœ… AIç”Ÿæˆäº†ä»»åŠ¡è®¡åˆ’ã€‚\n```json\n{plan_json_str}\n```\n")
        
        try:
            plan = json.loads(plan_json_str)
        except json.JSONDecodeError:
            log_to_job(job_id, f"âŒ JSONè§£æå¤±è´¥ï¼ŒåŸå§‹å­—ç¬¦: {plan_json_str}\n", "error")
            return
    else:
        log_to_job(job_id, f"âš ï¸ AIæœªç”Ÿæˆç»“æ„åŒ–è®¡åˆ’ï¼Œ{plan_json_str}\n", "done") # Treat as done if no plan
        return

    # --- Phase 2: Execution ---
    tasks = plan.get("tasks", [])
    global_steps = plan.get("global_steps", [])

    if not tasks and not global_steps:
        log_to_job(job_id, "è®¡åˆ’ä¸­æ²¡æœ‰å‘ç°ä»»ä½•æœ‰æ•ˆä»»åŠ¡æˆ–å…¨å±€æ­¥éª¤ã€‚", "done")
        return

    log_to_job(job_id, "\n### é˜¶æ®µäºŒï¼šä»»åŠ¡æ‰§è¡Œ\n")

    for i, task in enumerate(tasks):
        source_file = task.get("source_file", "æœªçŸ¥æ–‡ä»¶")
        log_to_job(job_id, f"å¥½çš„ï¼Œæˆ‘ä»¬å¼€å§‹å¤„ç†ç¬¬ {i+1} ä¸ªä»»åŠ¡ï¼Œç›®æ ‡æ–‡ä»¶æ˜¯ `{source_file}`ã€‚\n")

        for step_idx, step in enumerate(task.get("steps", [])):
            tool_name = step.get("tool")
            params = step.get("params", {})
            
            tool_translation = {
                "copy_to_temp": "å¤åˆ¶æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•",
                "extract_video_audio": "æå–è§†é¢‘å’ŒéŸ³é¢‘æµ",
                "extract_subtitle": "æå–å­—å¹•æ–‡ä»¶",
                "sync_subtitles": "åŒæ­¥å­—å¹•æ—¶é—´è½´",
            }
            step_description = tool_translation.get(tool_name, f"æ‰§è¡Œ `{tool_name}`")
            log_to_job(job_id, f"\n- **ç¬¬ {step_idx + 1} æ­¥**: {step_description}...")

            if tool_name in tool_map:
                try:
                    tool_function = tool_map[tool_name]
                    result = tool_function.invoke(params)
                    
                    if isinstance(result, dict) and result.get("status") == "success":
                        log_to_job(job_id, " âœ… æˆåŠŸï¼")
                        if "message" in result:
                             log_to_job(job_id, f" `{result['message']}`")
                    elif isinstance(result, dict) and result.get("status") == "error":
                        error_message = result.get('message', 'æœªçŸ¥é”™è¯¯')
                        log_to_job(job_id, f" âŒ å¤±è´¥ã€‚é”™è¯¯ä¿¡æ¯: {error_message}")
                        # Optionally break the loop on error? For now, we continue or break task
                        break 
                    else:
                        result_str = json.dumps(result, ensure_ascii=False, indent=2) if isinstance(result, dict) else str(result)
                        log_to_job(job_id, f" âœ… æ“ä½œå®Œæˆï¼Œè¿”å›ä¿¡æ¯ï¼š\n```{result_str}```")
                except Exception as e:
                    log_to_job(job_id, f" âŒ å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
                    break
            else:
                log_to_job(job_id, f" âš ï¸ **è­¦å‘Š**: æœªæ‰¾åˆ°åä¸º `{tool_name}` çš„å·¥å…·ã€‚")
        
        log_to_job(job_id, "\n\n---\n")

    # Global Steps
    if global_steps:
        log_to_job(job_id, "\n### ğŸ§¹ å…¨å±€æ¸…ç†é˜¶æ®µ\n")
        for step_idx, step in enumerate(global_steps):
            tool_name = step.get("tool")
            params = step.get("params", {})
            
            tool_translation = {
                "cleanup_temp": "æ¸…ç†ä¸´æ—¶ç›®å½•",
                "cleanup_subtitle": "æ¸…ç†å­—å¹•æ–‡ä»¶"
            }
            step_description = tool_translation.get(tool_name, f"æ‰§è¡Œ `{tool_name}`")
            log_to_job(job_id, f"\n- **å…¨å±€æ­¥éª¤ {step_idx + 1}**: {step_description}...")
            
            if tool_name in tool_map:
                try:
                    tool_function = tool_map[tool_name]
                    result = tool_function.invoke(params)
                    if isinstance(result, dict) and result.get("status") == "success":
                        log_to_job(job_id, " âœ… æˆåŠŸï¼")
                    elif isinstance(result, dict) and result.get("status") == "error":
                        log_to_job(job_id, f" âŒ å¤±è´¥: {result.get('message')}")
                    else:
                         log_to_job(job_id, f" âœ… å®Œæˆ")
                except Exception as e:
                    log_to_job(job_id, f" âŒ é”™è¯¯: {e}")
            else:
                log_to_job(job_id, f" âš ï¸ æœªçŸ¥å·¥å…·: {tool_name}")

    log_to_job(job_id, "æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæ¯•ï¼", "done")


# --- Gradio Interface ---
with gr.Blocks(
    title="âœï¸ å­—å¹•æ—¶é—´çº¿æ ¡æ­£åŠ©æ‰‹",
    theme="soft",
    css="""
    .markdown-container {
        padding: 10px 5px !important;
    }
    .markdown-label {
        font-weight: bold;
        margin-bottom: 10px;
        display: block;
    }
""",
) as app:

    gr.Markdown("# âœï¸ å­—å¹•æ—¶é—´çº¿æ ¡æ­£åŠ©æ‰‹\nè·Ÿæˆ‘è¯´æƒ³è¦æ ¡æ­£å­—å¹•çš„è§†é¢‘åç§°ï¼Œæˆ‘å°†ä¸ºä½ æœç´¢å¹¶è¿›è¡Œæå–éŸ³é¢‘æµã€å­—å¹•æ–‡ä»¶å’Œæ—¶é—´çº¿çš„æ ¡æ­£ã€ä»¥åŠåç»­çš„æ¸…ç†å·¥ä½œ...")

    # State to store the current job ID
    job_id_state = gr.State("")

    with gr.Row():
        with gr.Column(scale=2):
            chatbot = gr.Chatbot(label="å¯¹è¯çª—å£", height=500, show_copy_button=True)
            
            with gr.Row():
                user_input = gr.Textbox(
                    show_label=False, 
                    placeholder="æŒ‡ä»¤å¦‚ï¼š'å¸®æˆ‘åŒæ­¥ç‹‚é£™ç¬¬ä¸€é›†å­—å¹•æ—¶é—´è½´...'", 
                    scale=8, 
                    container=False
                )
                submit_btn = gr.Button("å‘é€", scale=1, variant="primary")
                clear_btn = gr.Button("æ¸…é™¤", scale=1, variant="secondary")

        with gr.Column(scale=1):
            gr.HTML("<div class='markdown-label'>ğŸ“ å¤„ç†æ­¥éª¤</div>")
            log_display = gr.Markdown(elem_classes=["markdown-container"], elem_id="log-box", height=400)
            
            # è‡ªåŠ¨æ»šåŠ¨è„šæœ¬
            scroll_js = """
            function() {
                const el = document.querySelector('#log-box .markdown-container');
                if (el) {
                    const isNearBottom = el.scrollHeight - el.scrollTop - el.clientHeight < 10;
                    if (isNearBottom) {
                        setTimeout(() => { el.scrollTop = el.scrollHeight; }, 50);
                    }
                }
            }
            """
            log_display.change(fn=None, inputs=None, outputs=None, js=scroll_js)

    def start_task(message: str, history: List[Tuple[str, str]]):
        """
        Starts the background task and returns the initial UI state.
        """
        if not message.strip():
            return history, "", gr.update(), gr.update(), gr.update(), ""

        # Create a new Job
        new_job_id = str(uuid.uuid4())
        with JOBS_LOCK:
            JOBS[new_job_id] = {
                "status": "running",
                "logs": [],
                "final_response": None,
                "created_at": time.time()
            }

        # Update chat history with user message
        history.append((message, "ğŸš€ ä»»åŠ¡å·²æäº¤è‡³åå°ï¼Œæ­£åœ¨å¤„ç†ä¸­..."))

        # Start the background thread
        threading.Thread(
            target=background_task_runner, 
            args=(new_job_id, message, history[:-1]), # Pass history excluding current new message
            daemon=True
        ).start()

        # Return updated UI state
        # Disable inputs while processing
        return (
            history, 
            "", 
            gr.update(interactive=False), 
            gr.update(interactive=False), 
            gr.update(interactive=False),
            new_job_id # Set the state
        )

    def monitor_task(job_id: str, history: List[Tuple[str, str]]):
        """
        Generator that yields log updates for the given job_id.
        """
        if not job_id:
            yield history, "", gr.update(interactive=True), gr.update(interactive=True), gr.update(interactive=True)
            return

        last_log_count = 0
        
        while True:
            job_data = None
            with JOBS_LOCK:
                job_data = JOBS.get(job_id)
            
            if not job_data:
                # Job not found
                yield history, "âš ï¸ æ‰¾ä¸åˆ°ä»»åŠ¡ä¿¡æ¯ã€‚", gr.update(interactive=True), gr.update(interactive=True), gr.update(interactive=True)
                break
            
            # Get current logs
            current_logs = job_data["logs"]
            full_log_text = "".join(current_logs)
            
            # Check status
            status = job_data.get("status", "running")
            
            if status in ["done", "error"]:
                # Update final chat message
                final_resp = job_data.get("final_response", "ä»»åŠ¡ç»“æŸ")
                # Update the last AI message in history
                if history:
                    history[-1] = (history[-1][0], final_resp)
                
                yield history, full_log_text, gr.update(interactive=True), gr.update(interactive=True), gr.update(interactive=True)
                break
            else:
                # Still running, just update logs
                # Only yield if logs have changed to save bandwidth/rendering? 
                # Gradio handles frequent yields okay, but checking count is better.
                if len(current_logs) > last_log_count:
                    yield history, full_log_text, gr.update(interactive=False), gr.update(interactive=False), gr.update(interactive=False)
                    last_log_count = len(current_logs)
            
            time.sleep(0.5) # Poll interval

    # Wire up events
    # 1. Submit -> Start Task (updates history, disables inputs, sets job_id)
    # 2. Then -> Monitor Task (reads job_id, updates logs and history, re-enables inputs when done)
    
    submit_event = submit_btn.click(
        start_task,
        inputs=[user_input, chatbot],
        outputs=[chatbot, user_input, submit_btn, clear_btn, user_input, job_id_state] # Note: user_input listed twice to clear it and disable it? Actually outputs map positionally.
        # wait, start_task outputs: history, user_input_val, submit_interactive, clear_interactive, user_interactive, job_id
    )
    
    submit_event.then(
        monitor_task,
        inputs=[job_id_state, chatbot],
        outputs=[chatbot, log_display, submit_btn, clear_btn, user_input]
    )

    # Handle 'Enter' key in textbox
    enter_event = user_input.submit(
        start_task,
        inputs=[user_input, chatbot],
        outputs=[chatbot, user_input, submit_btn, clear_btn, user_input, job_id_state]
    )
    
    enter_event.then(
        monitor_task,
        inputs=[job_id_state, chatbot],
        outputs=[chatbot, log_display, submit_btn, clear_btn, user_input]
    )

    clear_btn.click(
        fn=lambda: ([], ""),
        inputs=[],
        outputs=[chatbot, log_display]
    )

if __name__ == "__main__":
    app.launch(server_name="0.0.0.0", server_port=80)
